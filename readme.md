# AWS S3 to Azure Blob Storage Migration Tool

> **ðŸ“ Note**: This documentation was generated by GitHub Copilot. All steps, configurations, and code examples should be thoroughly tested and verified in your specific environment before use in production. Always follow your organization's security policies and best practices.

This tool provides a simple way to transfer files from AWS S3 to Azure Blob Storage.

- It discovers **all S3 buckets** in the AWS account.
- For each bucket, it ensures an Azure Blob **container exists** (one container per bucket, using the exact bucket name).
- It then copies all objects in that bucket into the matching container.

Azure authentication uses **the current user credential** via `DefaultAzureCredential` (no SAS tokens).

## Prerequisites

- Python 3.6 or higher
- Required Python packages:
  - `boto3` (AWS SDK for Python)
  - `azure-storage-blob` (Azure Storage SDK)
   - `azure-identity` (Azure identity/auth SDK)

## Quick Start with Azure Cloud Shell

The easiest way to get started is to use the automated setup script in Azure Cloud Shell:

### Option 1: Using the Setup Script (Recommended for Azure Cloud Shell)

1. **Open Azure Cloud Shell**
   - Go to [Azure Portal](https://portal.azure.com/)
   - Click the Cloud Shell icon in the top navigation bar
   - Select Bash environment

2. **Download and run the setup script**
   ```bash
   # Download the script
   wget https://raw.githubusercontent.com/ahmedsza/awss3toazureblobcopy/main/setup-and-run.sh
   
   # Review the script (optional but recommended)
   cat setup-and-run.sh
   
   # Make it executable and run with your parameters
   chmod +x setup-and-run.sh
   ./setup-and-run.sh --account-name yourstorageaccount --region us-east-1
   ```
   
   **Alternative (Quick but less secure):**
   If you trust the source, you can pipe directly to bash:
   ```bash
   curl -sL https://raw.githubusercontent.com/ahmedsza/awss3toazureblobcopy/main/setup-and-run.sh | bash -s -- --account-name yourstorageaccount --region us-east-1
   ```
   âš ï¸ **Security Note**: Piping scripts from the internet directly to bash can be risky. It's recommended to download and inspect the script first.

The script will automatically:
- Clone this repository
- Set up a Python virtual environment
- Install all required dependencies
- Run the copy tool with your provided parameters

### Option 2: Manual Installation

1. Clone or download this repository
2. Install the required dependencies:

```bash
pip install -r requirements.txt
```

Or install packages individually:

```bash
pip install boto3 azure-storage-blob azure-identity
```

## Features

- Discovers all S3 buckets
- Ensures a matching Azure Blob container exists for each bucket (creates it if missing)
- Copies all objects to Azure Blob Storage (streamed from S3; no temp files)
- Automatically overwrites blobs by default
- Works with both command-line arguments and environment variables
- Provides summary statistics

## Configuration Options

You can provide configuration parameters using either environment variables or command-line arguments:

| Parameter | Environment Variable | Command-line Option | Description |
|-----------|---------------------|---------------------|-------------|
| Azure Storage Account Name | `AZURE_STORAGE_ACCOUNT_NAME` | `--account-name` | Storage account name (e.g., `mystorageacct`) |
| Azure Storage Account URL | `AZURE_STORAGE_ACCOUNT_URL` | `--account-url` | Full blob endpoint (optional). Example: `https://mystorageacct.blob.core.windows.net` |
| AWS Region (hint) | `AWS_REGION` | `--region` | Region hint used during discovery (bucket operations use each bucketâ€™s actual region) |
| AWS Access Key ID | `AWS_ACCESS_KEY_ID` | `--access-key` | AWS access key ID (optional if using AWS default credentials) |
| AWS Secret Access Key | `AWS_SECRET_ACCESS_KEY` | `--secret-key` | AWS secret access key (optional if using AWS default credentials) |
| AWS Session Token | `AWS_SESSION_TOKEN` | `--session-token` | AWS session token (optional) |

## Usage Examples

### Using Command-line Arguments

```bash
python copyawstoazure.py --account-name yourstorageaccount --region us-east-1
```

### Using the Azure Cloud Shell Setup Script

The `setup-and-run.sh` script provides the most convenient way to run this tool in Azure Cloud Shell:

```bash
# Clone the repository first (if not already done)
git clone https://github.com/ahmedsza/awss3toazureblobcopy.git
cd awss3toazureblobcopy

# Run the setup script with your parameters
bash setup-and-run.sh --account-name yourstorageaccount --region us-east-1
```

The script accepts all the same parameters as `copyawstoazure.py`. You can also use environment variables:

```bash
# Set environment variables
export AZURE_STORAGE_ACCOUNT_NAME="yourstorageaccount"
export AWS_REGION="us-east-1"
export AWS_ACCESS_KEY_ID="YOUR_ACCESS_KEY"
export AWS_SECRET_ACCESS_KEY="YOUR_SECRET_KEY"

# Run the setup script
bash setup-and-run.sh
```

#### What the Setup Script Does

1. Clones or updates the repository (if run from outside the repo directory)
2. Creates a Python virtual environment (if not already present)
3. Installs all required dependencies from `requirements.txt`
4. Verifies Azure CLI authentication status
5. Runs `copyawstoazure.py` with your provided parameters
6. Provides colored output to track progress

### Using Environment Variables (PowerShell)

```powershell
# Azure: uses current user credential; make sure you're logged in
az login

$env:AZURE_STORAGE_ACCOUNT_NAME = "yourstorageaccount"

# AWS: either set keys OR rely on your existing AWS CLI/profile config
$env:AWS_REGION = "us-east-1"
$env:AWS_ACCESS_KEY_ID = "YOUR_ACCESS_KEY"
$env:AWS_SECRET_ACCESS_KEY = "YOUR_SECRET_ACCESS_KEY"

python copyawstoazure.py
```

### Using Environment Variables (Bash)

```bash
export S3_BUCKET="your-bucket-name"
export AWS_REGION="us-east-1"
export AWS_ACCESS_KEY_ID="YOUR_ACCESS_KEY"
export AWS_SECRET_ACCESS_KEY="YOUR_SECRET_KEY"
export AZURE_BLOB_SAS_URL="https://youraccount.blob.core.windows.net/container?sv=..."

python reads3_copyazure.py
```

## Getting Your Credentials

### AWS Access Keys

There are two approaches to obtain AWS access keys. **We strongly recommend using IAM users instead of root user access keys for security reasons.**

#### Option 1: IAM User Access Keys (Recommended)

1. **Sign in to AWS Console**
   - Go to [AWS Management Console](https://aws.amazon.com/console/)
   - Sign in with your AWS account

2. **Navigate to IAM (Identity and Access Management)**
   - In the AWS Console, search for "IAM" or go to Services > Security, Identity, & Compliance > IAM

3. **Create or Select a User**
   - Click on "Users" in the left sidebar
   - Either select an existing user or click "Add users" to create a new one
   - For new users, choose "Programmatic access" as the access type

4. **Attach Permissions**
   - Attach the `AmazonS3ReadOnlyAccess` policy (or a custom policy with S3 read permissions)
   - For the specific bucket, you can create a custom policy like:
   ```json
   {
       "Version": "2012-10-17",
       "Statement": [
           {
               "Effect": "Allow",
               "Action": [
                   "s3:GetObject",
                   "s3:ListBucket"
               ],
               "Resource": [
                   "arn:aws:s3:::your-bucket-name",
                   "arn:aws:s3:::your-bucket-name/*"
               ]
           }
       ]
   }
   ```

5. **Download or Copy Keys**
   - After creating the user, you'll see the Access Key ID and Secret Access Key
   - **Important**: Save these immediately as the Secret Access Key won't be shown again
   - Consider downloading the CSV file for safekeeping

#### Option 2: Root User Access Keys (NOT RECOMMENDED)

> **âš ï¸ SECURITY WARNING**: Using root user access keys is strongly discouraged by AWS and security best practices. Root users have unrestricted access to all AWS services and resources. Use this method only for testing purposes and delete the keys immediately after use.

**Why You Shouldn't Use Root Access Keys:**
- **Unlimited Permissions**: Root users have access to everything in your AWS account
- **Security Risk**: If compromised, attackers have full control of your AWS account
- **No Permission Boundaries**: Cannot limit what the root user can access
- **Compliance Issues**: Violates most security compliance frameworks
- **AWS Best Practices**: AWS explicitly recommends against this approach

**If you absolutely must use root access keys (for testing only):**

1. **Sign in as Root User**
   - Go to [AWS Management Console](https://aws.amazon.com/console/)
   - Sign in using your root email address and password (not an IAM user)

2. **Navigate to Security Credentials**
   - Click on your account name in the top right corner
   - Select "Security credentials" from the dropdown menu

3. **Access Keys Section**
   - Scroll down to the "Access keys" section
   - AWS may show a warning about security risks
   - Click "Create access key"
   - Select "Command Line Interface (CLI)" as the use case
   - Check the confirmation box acknowledging the security risks

4. **Download Keys Immediately**
   - Copy the Access Key ID and Secret Access Key immediately
   - Download the CSV file for backup
   - **Critical**: Store these securely as the Secret Access Key won't be shown again

5. **Immediate Security Actions (REQUIRED)**
   - Enable MFA (Multi-Factor Authentication) on your root account if not already enabled
   - Set up AWS CloudTrail to monitor all API calls
   - Consider creating an IAM user immediately and deleting the root access keys
   - Review AWS account activity regularly

**Better Alternatives to Root Access Keys:**

1. **IAM User with PowerUser Policy** (if you need broad permissions):
   ```json
   {
       "Version": "2012-10-17",
       "Statement": [
           {
               "Effect": "Allow",
               "Action": "*",
               "Resource": "*"
           },
           {
               "Effect": "Deny",
               "Action": [
                   "iam:*",
                   "organizations:*",
                   "account:*"
               ],
               "Resource": "*"
           }
       ]
   }
   ```

2. **IAM User with S3 Full Access** (for S3 operations only):
   ```json
   {
       "Version": "2012-10-17",
       "Statement": [
           {
               "Effect": "Allow",
               "Action": "s3:*",
               "Resource": "*"
           }
       ]
   }
   ```

3. **IAM Roles** (for applications running on AWS services like EC2)

**Root Access Key Security Checklist:**
- [ ] Enable MFA on root account
- [ ] Set up AWS CloudTrail
- [ ] Set up AWS Config for compliance monitoring
- [ ] Create IAM users for daily operations
- [ ] Delete root access keys after testing
- [ ] Set up billing alerts
- [ ] Review AWS Trusted Advisor security recommendations

### Azure Blob Storage SAS Token

To generate a SAS (Shared Access Signature) token for your Azure Blob Storage container:

#### Method 1: Using Azure Portal

1. **Sign in to Azure Portal**
   - Go to [Azure Portal](https://portal.azure.com/)
   - Sign in with your Azure account

2. **Navigate to Your Storage Account**
   - Search for "Storage accounts" or go to All services > Storage > Storage accounts
   - Click on your storage account name

3. **Go to Your Container**
   - In the left sidebar, click on "Containers" under Data storage
   - Click on the container you want to create a SAS token for

4. **Generate SAS Token**
   - Click on "Generate SAS" in the container's toolbar
   - Configure the permissions:
     - **Permissions**: Select "Read", "Add", "Create", "Write", "List" (minimum required)
     - **Start time**: Set to current time or earlier
     - **Expiry time**: Set to when you want the token to expire
     - **Allowed protocols**: HTTPS only (recommended)
   - Click "Generate SAS token and URL"

5. **Copy the SAS URL**
   - Copy the "Blob SAS URL" (this is what you'll use for the `--azure-sas` parameter)
   - The URL format will be: `https://youraccount.blob.core.windows.net/container?sv=...&sig=...`

#### Method 2: Using Azure CLI

```bash
# Install Azure CLI if not already installed
# https://docs.microsoft.com/en-us/cli/azure/install-azure-cli

# Login to Azure
az login

# Generate SAS token (replace with your values)
az storage container generate-sas \
    --account-name yourstorageaccount \
    --name yourcontainer \
    --permissions racwdl \
    --expiry 2025-12-31T23:59:59Z \
    --https-only \
    --output tsv
```

#### Method 3: Using Azure Storage Explorer

1. Download and install [Azure Storage Explorer](https://azure.microsoft.com/en-us/features/storage-explorer/)
2. Connect to your Azure account
3. Navigate to your storage account > Blob Containers > Your container
4. Right-click on the container and select "Get Shared Access Signature..."
5. Configure permissions and expiry time
6. Click "Create" and copy the URL

### Security Best Practices for Credentials

1. **AWS Credentials**:
   - Use IAM users with minimal required permissions
   - Enable MFA (Multi-Factor Authentication) on your AWS account
   - Regularly rotate access keys
   - Never commit credentials to version control
   - Consider using AWS IAM roles for EC2 instances instead of access keys

2. **Azure SAS Tokens**:
   - Set appropriate expiration times (not too long)
   - Use HTTPS-only protocols
   - Grant only the minimum required permissions
   - Monitor access logs for unauthorized usage
   - Consider using Azure Key Vault for storing SAS tokens in production

## Azure Blob Storage Security Considerations

1. **SAS Token Permissions**: Ensure your SAS token has the appropriate permissions (write/create)
2. **SAS Token Expiration**: Be aware of the expiration time of your SAS token
3. **Token Protection**: Keep your SAS tokens secure and avoid hardcoding them
4. **Least Privilege**: Generate SAS tokens with the minimum permissions needed
5. **Use Key Vault**: For production scenarios, consider storing SAS tokens in Azure Key Vault

## Troubleshooting

### Common Issues

- **Invalid SAS URL**: Ensure your SAS URL is properly formatted and has not expired
- **S3 Access Denied**: Verify your AWS credentials and bucket permissions
- **File Not Found**: Check if the S3 bucket contains the expected files
- **Network Errors**: Verify your network connectivity to both AWS and Azure services

### Checking Azure Blob Upload Status

To verify files were uploaded to your Azure Blob Storage container, use the Azure Portal or Azure Storage Explorer.

## Best Practices

- **For Production Environments**: 
  - Implement logging with Azure Monitor
  - Consider using Azure Key Vault for credential storage
  - Add retry policies for transient failures
  - Use Managed Identities instead of SAS tokens when possible
- **For Large Datasets**: 
  - Consider Azure Data Factory for enterprise-scale data migration
  - Monitor network bandwidth usage
  - Schedule migrations during off-peak hours

## Limitations

- The script downloads files to temporary storage, so your system must have enough disk space
- Performance depends on network bandwidth between your system, AWS, and Azure
- Files are uploaded with their original names, preserving only the filename (not the full path)